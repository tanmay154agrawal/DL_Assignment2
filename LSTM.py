# -*- coding: utf-8 -*-
"""m22ma011_DL_Assignment2_q1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19loXsL4Sq6V1mISpbOIyDLOfBtEtivGL
"""

import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
import random

dataset = pd.read_csv("hi.translit.sampled.train.tsv",sep='\t')
test=pd.read_csv("hi.translit.sampled.test.tsv",sep='\t')

dataset.head()

test.head()

print(dataset.dtypes)

dataset['an']=dataset['an'].astype(str)
test['ank']=test['ank'].astype(str)

len(dataset)

token_df=pd.DataFrame()
token_df1=pd.DataFrame()

def tokenize_string(string):
    tokens = list(string)
    return tokens

def tokenize_string_eng(string):
    chars = [char for char in string]
    return chars

token_df['Tokens_hindi'] = dataset['अं'].apply(tokenize_string)
token_df['Tokens_english']=dataset['an'].apply(tokenize_string_eng)

token_df1['Tokens_hindi'] = test['अंक'].apply(tokenize_string)
token_df1['Tokens_english']=test['ank'].apply(tokenize_string_eng)

token_df.head()

eos_token = '<eos>'
start_token = '<sos>'
token_df['Tokens_hindi'] = token_df['Tokens_hindi'].apply(lambda x: [start_token] + x + [eos_token])
token_df['Tokens_english'] = token_df['Tokens_english'].apply(lambda x:  [start_token] + x + [eos_token])

token_df1['Tokens_hindi'] = token_df1['Tokens_hindi'].apply(lambda x: [start_token] + x + [eos_token])
token_df1['Tokens_english'] = token_df1['Tokens_english'].apply(lambda x:  [start_token] + x + [eos_token])

from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer(analyzer='char')

# fit_transform the tokenized text to create a matrix of token counts
token_matrix = vectorizer.fit_transform(token_df['Tokens_english'].apply(lambda x: ''.join(x)))
eng_vocab=vectorizer.vocabulary_
print(eng_vocab)

eng_vocab.update({'<sos>': 63})
eng_vocab.update({'<eos>': 64})

token_matrix = vectorizer.fit_transform(token_df['Tokens_hindi'].apply(lambda x: ''.join(x)))
hindi_vocab=vectorizer.vocabulary_
print(hindi_vocab)

hindi_vocab.update({'<sos>': 63})
hindi_vocab.update({'<eos>': 64})

token_df['encoded_tokens_eng'] = token_df['Tokens_english'].apply(lambda x: [eng_vocab[token] for token in x])
token_df['encoded_tokens_hindi'] = token_df['Tokens_hindi'].apply(lambda x: [hindi_vocab[token] for token in x])

token_df1['encoded_tokens_eng'] = token_df1['Tokens_english'].apply(lambda x: [eng_vocab[token] for token in x])
token_df1['encoded_tokens_hindi'] = token_df1['Tokens_hindi'].apply(lambda x: [hindi_vocab[token] for token in x])

token_df.head()

print(token_df['encoded_tokens_eng'].apply(len).max())
print(token_df['encoded_tokens_hindi'].apply(len).max())

print(token_df1['encoded_tokens_eng'].apply(len).max())
print(token_df1['encoded_tokens_hindi'].apply(len).max())

max_length=24
padding_token=100
token_df['padded_tokens_eng'] = token_df['encoded_tokens_eng'].apply(lambda x: x + [padding_token]*(max_length - len(x)) if len(x) < max_length else x[:max_length])
token_df['padded_tokens_hindi'] = token_df['encoded_tokens_hindi'].apply(lambda x: x + [padding_token]*(max_length - len(x)) if len(x) < max_length else x[:max_length])

token_df1['padded_tokens_eng'] = token_df1['encoded_tokens_eng'].apply(lambda x: x + [padding_token]*(max_length - len(x)) if len(x) < max_length else x[:max_length])
token_df1['padded_tokens_hindi'] = token_df1['encoded_tokens_hindi'].apply(lambda x: x + [padding_token]*(max_length - len(x)) if len(x) < max_length else x[:max_length])

train_data=[]
for index, row in token_df.iterrows():
    train_data.append((torch.tensor(row['padded_tokens_eng']).unsqueeze(0),torch.tensor(row['padded_tokens_hindi']).unsqueeze(0)))

test_data=[]
for index, row in token_df1.iterrows():
    test_data.append((torch.tensor(row['padded_tokens_eng']).unsqueeze(0),torch.tensor(row['padded_tokens_hindi']).unsqueeze(0)))

from torch.utils.data import Dataset, DataLoader
dataloader = DataLoader(train_data, batch_size=64,drop_last=True,shuffle=True)
test_dataloader = DataLoader(test_data, batch_size=64,drop_last=True)

input_size_encoder = 101
encoder_embedding_size = 64
hidden_size = 64
num_layers = 3

input_size_decoder = 101
decoder_embedding_size = 64
hidden_size = 64
num_layers = 3
decoder_dropout = float(0.5)
output_size = 101

class EncoderLSTM(nn.Module):
  def __init__(self, input_size, embedding_size, hidden_size, num_layers):
    super(EncoderLSTM, self).__init__()
    # Size of the one hot vectors that will be the input to the encoder
    self.input_size = input_size
    # Output size of the word embedding NN
    self.embedding_size = embedding_size
    # Dimension of the NN's inside the lstm cell/ (hs,cs)'s dimension.
    self.hidden_size = hidden_size
    # Number of layers in the lstm
    self.num_layers = num_layers
    # Shape [input size, embedding dims]
    self.embedding = nn.Embedding(self.input_size, self.embedding_size)
    
    # Shape [embedding dims, hidden size, num layers]
    self.LSTM = nn.LSTM(self.embedding_size, hidden_size, num_layers)

  # Shape of x  [Sequence_length, batch_size]
  def forward(self, x):
    # Shape [Sequence_length , batch_size , embedding dims]
    # Shape [Sequence_length , batch_size , hidden_size]
    # Shape  [num_layers, batch_size size, hidden_size]
    x=self.embedding(x)
    # print(x.shape)
    outputs, (hidden_state, cell_state) = self.LSTM(x)
    return hidden_state, cell_state



encoder_lstm = EncoderLSTM(input_size_encoder, encoder_embedding_size,
                           hidden_size, num_layers).cuda()
print(encoder_lstm)

class DecoderLSTM(nn.Module):
  def __init__(self, input_size, embedding_size, hidden_size, num_layers, p, output_size):
    super(DecoderLSTM, self).__init__()

    # Size of the one hot vectors that will be the input to the encoder
    self.input_size = input_size

    # Output size of the word embedding NN
    self.embedding_size = embedding_size

    # Dimension of the NN's inside the lstm cell/ (hs,cs)'s dimension.
    self.hidden_size = hidden_size

    # Number of layers in the lstm
    self.num_layers = num_layers

    # Size of the one hot vectors that will be the output to the encoder (English Vocab Size)
    self.output_size = output_size

    # Regularization parameter
    self.dropout = nn.Dropout(p)
    self.tag = True

    # Shape  [input size, embedding dims]
    self.embedding = nn.Embedding(self.input_size, self.embedding_size)

    # Shape [embedding dims, hidden size, num layers]
    self.LSTM = nn.LSTM(self.embedding_size, hidden_size, num_layers, dropout = p)

    # Shape [embedding dims, hidden size, num layers]
    self.fc = nn.Linear(self.hidden_size, self.output_size)

  # Shape of x  [batch_size]
  def forward(self, x, hidden_state, cell_state):
    # Shape of x  [1, batch_size]
    x = x.unsqueeze(0)
    # print(x.shape)
    # Shape  [1, batch_size, embedding dims]
    embedding = self.dropout(self.embedding(x))
    # print(embedding.shape)
    # Shape [1, batch_size , hidden_size]
    # Shape  [num_layers, batch_size size, hidden_size] (passing encoder's hs, cs - context vectors)
    outputs, (hidden_state, cell_state) = self.LSTM(embedding, (hidden_state, cell_state))
    # print(outputs.shape)
    # Shape  [ 1, batch_size , output_size]
    predictions = self.fc(outputs)
    # print(predictions.shape)
    # Shape  [batch_size , output_size]
    predictions = predictions.squeeze(0)
    # print(predictions.shape)
    return predictions, hidden_state, cell_state


decoder_lstm = DecoderLSTM(input_size_decoder, decoder_embedding_size,
                           hidden_size, num_layers, decoder_dropout, output_size).cuda()
print(decoder_lstm)

class Seq2Seq(nn.Module):
  def __init__(self, Encoder_LSTM, Decoder_LSTM):
    super(Seq2Seq, self).__init__()
    self.Encoder_LSTM = Encoder_LSTM
    self.Decoder_LSTM = Decoder_LSTM

  def forward(self, source, target, tfr=0.5):
    # batch_size = source.shape[1]
    
    # Shape -  [(Sentence length English + some padding), Number of Sentences]
    target_len = target.shape[0]
    target_vocab_size = 101
    
    # Shape --> outputs 
    outputs = torch.zeros(target_len,64, target_vocab_size)
    # Shape [num_layers, batch_size size, hidden_size] (contains encoder's hs, cs - context vectors)
    hidden_state_encoder, cell_state_encoder = self.Encoder_LSTM(source)

    # Shape of x 
    x = target[0] # Trigger token <SOS>

    for i in range(1, 24):
      output, hidden_state_decoder, cell_state_decoder = self.Decoder_LSTM(x, hidden_state_encoder, cell_state_encoder)
      outputs[i] = output
      best_guess = output.argmax(1) # 0th dimension is batch size, 1st dimension is word embedding
      x = target[i] if random.random() < tfr else best_guess # Either pass the next word correctly from the dataset or use the earlier predicted word

    return outputs

model=Seq2Seq(encoder_lstm,decoder_lstm).cuda()
print(model)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

losslist=[]

from tqdm import tqdm
model.train(True)
for epoch in range(10):
  loss1=0
  for input , target in tqdm(dataloader):
    input = input.reshape(-1,64).cuda()
    target=target.cuda()
    target=target.reshape(-1,64)
    optimizer.zero_grad()   
    output = model(input, target).cuda()
    output=output
    # print(output.shape)
    output = output[1:].reshape(-1, output.shape[2])
    # print(output.shape)
    target = target[1:].reshape(-1)
    loss = criterion(output, target)
    loss.backward()
    optimizer.step()
    loss1+=loss
  losslist.append(loss1/len(dataloader))

losses=[]
for i in losslist:
 i=i.detach().cpu()
 losses.append(i)

import matplotlib.pyplot as plt
plt.plot(losses)
plt.xlabel('Epoch')
plt.ylabel('Losses')

total_correct = 0
total_samples = 0
model.eval()
with torch.no_grad():
  for input , target in test_dataloader:
    input = input.reshape(-1,64).cuda()
    target=target.cuda()
    target=target.reshape(-1,64)
    output = model(input, target).cuda()
    output = output[1:].reshape(-1, 101 , 64)
    target = target[1:].reshape(-1,64)
    _, predicted = torch.max(output.data, 1)
    total_samples += target.size(1)
    total_correct += (predicted == target).sum().item()
accuracy = total_correct / total_samples
print(accuracy)

